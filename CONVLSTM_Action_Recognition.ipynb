{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CONVLSTM Action Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb6F09ov3r6e"
      },
      "source": [
        "#!pip install wandb\n",
        "#import wandb\n",
        "#wandb.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS8uhihgiScX"
      },
      "source": [
        "#Importing Modules\n",
        "\n",
        "#Importing Keras and import sub-modules needed\n",
        "import keras\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model \n",
        "from keras.layers import *\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
        "\n",
        "#Importing miscallaneous modules\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        " #Importing sklearn modules to calculate different metrics and create different tables\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y3mgCGDjBWy"
      },
      "source": [
        "from google.colab import drive    #Access Google Drive which is used as location for all code, datasets and relevant files\n",
        "drive.mount('/gdrive')\n",
        "#go to root of Google Drive\n",
        "%cd /gdrive   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJPzfPkBj6_E"
      },
      "source": [
        "#Navigate to folder where all the datasets are \n",
        "%cd 'My Drive'   \n",
        "%cd 'Action Recognition'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_ePpVo3ilG5"
      },
      "source": [
        "data_dir = \"hmdb51/\" #Choose dataset by naming dataset folder name\n",
        "img_height , img_width = 64, 64 #Set pixel values for frames\n",
        "seq_len = 70 #Set number of frames/samples per video\n",
        "classes = [\"pullup\", \"punch\", \"dive\", \"fencing\", \"ride_bike\", \"golf\"] #Select class names from chosen dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhrxQkMQini-"
      },
      "source": [
        "def frames_extraction(video_path):                              #Helper function to extract frames from videos\n",
        "    frames_list = []                                            #Create empty list\n",
        "    \n",
        "    vidObj = cv2.VideoCapture(video_path)\n",
        "    count = 1                                                   #Counter variable used to keep track of number of frames extracted\n",
        " \n",
        "    while count <= seq_len: \n",
        "         \n",
        "        success, image = vidObj.read()                          #CV2 function used to read images from videos\n",
        "        if success:                                             #While end of video not reached\n",
        "            image = cv2.resize(image, (img_height, img_width))  #Extract frame and resize to the pixel values pre-set\n",
        "            frames_list.append(image)                           #Add to frame list\n",
        "            count += 1                                          #Increment count\n",
        "        else:\n",
        "            print(\"Defected frame\")                             #Print message if frame not able to be read\n",
        "            break\n",
        " \n",
        "            \n",
        "    return frames_list                                          #Exit once processing on current video is complete\n",
        " \n",
        "def create_data(input_dir):                                     #Create data function\n",
        "    X = []                                                      #Empty lists for videos and corresponding labels\n",
        "    Y = []\n",
        "     \n",
        "    classes_list = os.listdir(input_dir)                        #Get all folder names in dataset, i.e full class list of dataset (test purposes)\n",
        "    print (classes_list)                                        \n",
        "    for c in classes:                                           #Iterate through list of chosen classes\n",
        "        print(c)                                                \n",
        "        files_list = os.listdir(os.path.join(input_dir, c))     #Get list of all video names in current folder\n",
        "        for f in files_list:                                    #Iterate through this list\n",
        "           frames = frames_extraction(os.path.join(os.path.join(input_dir, c), f))    #Extract set number of frames from cuurent video\n",
        "           if len(frames) == seq_len:                           #Make sure desired number of frames was extracted\n",
        "             X.append(frames)\n",
        "             y = [0]*len(classes)                               #Add corrresponding video label class to list of labels\n",
        "             y[classes.index(c)] = 1\n",
        "             Y.append(y)\n",
        "     \n",
        "    X = np.asarray(X)                                           #Convert lists to numpy arrays\n",
        "    Y = np.asarray(Y)\n",
        "    return X, Y                                                 #Return final data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coS9nPAfiq6m"
      },
      "source": [
        "X, Y = create_data(data_dir)                                    #Fetch data for chosen dataset\n",
        "\n",
        "#print (X.shape)\n",
        "#print (Y.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX-a5MzBFUwm"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, shuffle=True, random_state=0) #Split video data and labels into test and train sets\n",
        "\n",
        "print (X_train.shape)\n",
        "print (y_train.shape)\n",
        "print (X_test.shape)\n",
        "print (y_test.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd5TfS2JitHO"
      },
      "source": [
        "model = Sequential()  #Model initiated and layers added whilst specifying hyperparameters\n",
        "\n",
        "model.add(ConvLSTM2D(filters = 64, kernel_size = (3, 3), return_sequences = False, data_format = \"channels_last\", input_shape = (seq_len, img_height, img_width, 3)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation=\"relu\"))\n",
        "model.add(Dense(256, activation=\"relu\"))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(6, activation = \"softmax\"))\n",
        "model.summary()  #Print summary of model\n",
        "\n",
        "opt = keras.optimizers.SGD(lr=0.001)    #Specify training algorithm and learning rate\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])  #Specify Loss and Accuracy metrics as well"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_BD4CUpiw3-"
      },
      "source": [
        "earlystop = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True) #Add early stopping by specifying patience value; wait for x amount of epochs where no improvement is seen before reverting to best weights\n",
        "callbacks = [earlystop]\n",
        " \n",
        "history = model.fit(x = X_train, y = y_train, epochs=40, batch_size = 8 , shuffle=True, validation_split=0.2, callbacks=callbacks) #Fit model, specify number of epochs, validation % of training set, batch size \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LIZvGCqizrk"
      },
      "source": [
        "model_evaluation_history = model.evaluate(X_test, y_test)  #Evaluate model on test set\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report #Produce report with extra metrics\n",
        "\n",
        "y_pred = model.predict(X_test, batch_size=4, verbose=1)\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis = 1)\n",
        "y_test = np.argmax(y_test, axis = 1)\n",
        " \n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzUR4P5rUAMN"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix  #Produce confusion matrix to show each class' performance\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print (cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPKkFVdZRdqg"
      },
      "source": [
        "def plot_metric(metric_name_1, metric_name_2, plot_name):\n",
        "                                                                        # Fetch loss/accuracy values\n",
        "  metric_value_1 = history.history[metric_name_1]\n",
        "  metric_value_2 = history.history[metric_name_2]\n",
        "                                                                        \n",
        "  epochs = range(len(metric_value_1))                                   # Get epochs\n",
        "                                                                        # Plot Graph\n",
        "  plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n",
        "  plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n",
        "                                                                      \n",
        "  plt.title(str(plot_name))\n",
        "                                                                     \n",
        "  plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHNIhkL3Re7Q"
      },
      "source": [
        "plot_metric('loss', 'val_loss', 'Total Loss vs Total Validation Loss')  #Plot loss/val loss graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXNYRrLcRgUY"
      },
      "source": [
        "plot_metric('accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')  #Plot accuracy/val accuracy graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpD5XzuI1o3Y"
      },
      "source": [
        "#evaluate on new different data set with similar classes\n",
        "\n",
        "data_dir2 = \"UCF50/\"  \n",
        " \n",
        "X1, Y1 = create_data(data_dir2) #use previous helper functions to extract frames from videos\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yM72YJY2F5s"
      },
      "source": [
        "Eval_Hist = model.evaluate(X1, Y1)  #evaluate on whole set from new dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-7bB82X8ZaO"
      },
      "source": [
        "Y2 = model.predict(X1, batch_size=4, verbose=1)  #produce extra metrics for prediction on new dataset\n",
        "\n",
        "Y2 = np.argmax(Y2, axis = 1)\n",
        "Y1 = np.argmax(Y1, axis = 1)\n",
        " \n",
        "print(classification_report(Y1, Y2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIMoS7Ca9Cmk"
      },
      "source": [
        "print(confusion_matrix(Y1, Y2))  #produce confusion matrix for prediction on new dataset"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}