{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Abi Model Random Search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s39yElYhs4N"
      },
      "source": [
        "#!pip install wandb\n",
        "#import wandb\n",
        "#wandb.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BbJWRXlJ-wd"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import pafy\n",
        "import random\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "!pip install -q -U keras-tuner\n",
        "from kerastuner.tuners import RandomSearch\n",
        "import keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgXc9OdQKYLE"
      },
      "source": [
        "seed_constant = 23\n",
        "np.random.seed(seed_constant)\n",
        "random.seed(seed_constant)\n",
        "tf.random.set_seed(seed_constant)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as2q_7uKKcul"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kttPfE9yKzmI"
      },
      "source": [
        "%cd 'My Drive'\n",
        "\n",
        "%cd 'Action Recognition'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE7MFdDmLCbT"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VktUVQiBeCrj"
      },
      "source": [
        "#Constants\n",
        "\n",
        "datasetName= 'hmdb51'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUvcHinhOwuP"
      },
      "source": [
        "image_height, image_width = 64, 64\n",
        "images_per_class = 8000\n",
        "dataset_directory = \"hmdb51\"\n",
        "classes_list = [\"golf\", \"eat\", \"dribble\", \"climb\", \"clap\", \"run\"]\n",
        "model_output_size = len(classes_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxnVgT1BPmGU"
      },
      "source": [
        "def frames_extraction(video_path):  #helper function to extract frames from videos\n",
        "    frames_list = []                #empty list for frames\n",
        "    video_reader = cv2.VideoCapture(video_path) #Read frames from video\n",
        "    while True:     #Iterate through frames\n",
        "        success, frame = video_reader.read() #Whilst frames are available\n",
        "        if not success:\n",
        "            break\n",
        "        resized_frame = cv2.resize(frame, (image_height, image_width))    #Resize to pre-defined pixel values\n",
        "        normalized_frame = resized_frame / 255                            #Normalize frames to range 0-1\n",
        "        frames_list.append(normalized_frame)                              #Add to frame list for this video\n",
        "    video_reader.release()                                                #Close and release contents\n",
        "    return frames_list    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu-vIhjfPyhk"
      },
      "source": [
        "def create_dataset():    #Create dataset function\n",
        "    temp_features = [] #empty list to hold each videos frames\n",
        "    features = []  #final list of frames will be in this list\n",
        "    labels = []  #Final list of labels will be in this list\n",
        "\n",
        "    for class_index, class_name in enumerate(classes_list):  #Iterate through chosen classes\n",
        "        print(f'Extracting Data of Class: {class_name}')        \n",
        "        files_list = os.listdir(os.path.join(dataset_directory, class_name)) #Got to class folder and get video list\n",
        "        for file_name in files_list:\n",
        "            video_file_path = os.path.join(dataset_directory, class_name, file_name)\n",
        "            frames = frames_extraction(video_file_path)   #Extract frames for current video\n",
        "            temp_features.extend(frames)      #Add to temp frames list\n",
        "\n",
        "        features.extend(random.sample(temp_features, images_per_class))       #Choose 8000 random frames from current class (all videos in class)\n",
        "        labels.extend([class_index] * images_per_class)                         #Assign 8000 labels correctly\n",
        "        temp_features.clear()\n",
        "\n",
        "    features = np.asarray(features)   #Convert both to numpy array\n",
        "    labels = np.array(labels)  \n",
        "    return features, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ditWFy33QCfw"
      },
      "source": [
        "features, labels = create_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Hpo22S7mIh"
      },
      "source": [
        "print (features.shape)\n",
        "print (labels.shape)\n",
        "#print (features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EobZUwegRvCL"
      },
      "source": [
        "one_hot_encoded_labels = to_categorical(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJtwnA1_R0cs"
      },
      "source": [
        "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.2, shuffle = True, random_state = seed_constant)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyo-l4nzR-F5"
      },
      "source": [
        "def create_model(hp):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Defining The Model Architecture\n",
        "    model.add(Conv2D(filters = hp.Int('Conv2D Units', min_value=32, max_value=256, step=32), kernel_size = (3, 3), activation = 'relu', input_shape = (image_height, image_width, 3)))\n",
        "    \n",
        "    for i in range(hp.Int('Conv2D Layers', 1, 4)):\n",
        "      model.add(Conv2D(filters = hp.Int('Conv2D Units', min_value=32, max_value=256, step=32), kernel_size = (3, 3), activation = 'relu'))\n",
        "    \n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    \n",
        "    for i in range(hp.Int('Fully Connected Layers', 1, 3)):\n",
        "      model.add(Dense(units=hp.Int('Dense Units',min_value=32,max_value=512,step=32), activation = 'relu'))\n",
        "    \n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "    model.add(Dense(model_output_size, activation = 'softmax'))\n",
        "\n",
        "    # Printing the models summary\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "        loss='categorical_crossentropy',\n",
        "        )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFC3vC7SyFzD"
      },
      "source": [
        "tuner = RandomSearch(\n",
        "    create_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='project',\n",
        "    project_name='HAR')\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "tuner.search(features_train, labels_train,\n",
        "             epochs=40,\n",
        "             validation_split=0.2, callbacks=[EarlyStopping(patience=7)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVOWrLscO5Xm"
      },
      "source": [
        "tuner.search_space_summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR93CcqK6NEH"
      },
      "source": [
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}